{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX2j4mvDrskklcPayur/EM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/borisk8180-glitch/Bootcamp25/blob/main/Di-bootcamp/week3/day2/XP/XPw3d3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7YOSOeEOfiB"
      },
      "outputs": [],
      "source": [
        "# Import necessary library\n",
        "import pandas as pd\n",
        "\n",
        "# STEP 1: Load the Titanic dataset\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "df = pd.read_csv(\"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Display the initial number of rows\n",
        "print(\"Number of rows before duplicate removal:\", len(df))"
      ],
      "metadata": {
        "id": "rSHCDi3fOphG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Identify duplicate rows\n",
        "# duplicated() returns a boolean Series where True means the row is a duplicate\n",
        "duplicates = df.duplicated()\n",
        "\n",
        "# Count how many duplicates were found\n",
        "print(\"Number of duplicate rows found:\", duplicates.sum())"
      ],
      "metadata": {
        "id": "C5_9P6kkOsbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Remove duplicate rows\n",
        "# drop_duplicates() removes all rows that are duplicates\n",
        "df_cleaned = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "lWzLI3mrOvjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Verify the removal of duplicates\n",
        "print(\"Number of rows after duplicate removal:\", len(df_cleaned))\n",
        "\n",
        "# Optional: Save the cleaned dataset to a new CSV file\n",
        "df_cleaned.to_csv(\"titanic_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"Duplicate removal complete. Cleaned dataset saved as 'titanic_cleaned.csv'.\")\n"
      ],
      "metadata": {
        "id": "R9y05THfOx8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# # STEP 1: Load the Titanic dataset\n",
        "# df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# STEP 2: Identify columns with missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "LyuEWQiLQhxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STRATEGIES FOR HANDLING MISSING DATA\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# STRATEGY A: Removal (drop rows with missing values in critical columns)\n",
        "# Example: Drop rows where 'Embarked' is missing (few rows only)\n",
        "df_removed = df.dropna(subset=['Embarked'])\n",
        "print(\"\\nAfter removing rows with missing 'Embarked':\", len(df_removed))"
      ],
      "metadata": {
        "id": "t0sQHfBoQ7zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STRATEGY B: Filling with a constant value\n",
        "# Example: Fill missing 'Cabin' values with 'Unknown'\n",
        "df_filled_constant = df.copy()\n",
        "df_filled_constant['Cabin'] = df_filled_constant['Cabin'].fillna(\"Unknown\")\n",
        "print(\"\\nNumber of missing 'Cabin' after fillna:\", df_filled_constant['Cabin'].isnull().sum())"
      ],
      "metadata": {
        "id": "-AL1vtpYRCaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STRATEGY C: Filling with statistical measures (imputation)\n",
        "# Example: Fill missing 'Age' values with the median\n",
        "df_filled_stat = df.copy()\n",
        "df_filled_stat['Age'] = df_filled_stat['Age'].fillna(df_filled_stat['Age'].median())\n",
        "print(\"\\nNumber of missing 'Age' after median imputation:\", df_filled_stat['Age'].isnull().sum())"
      ],
      "metadata": {
        "id": "LfM-NMX5RCkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STRATEGY D: Using scikit-learn SimpleImputer\n",
        "# Example: Fill missing 'Fare' with the mean using SimpleImputer\n",
        "df_imputed = df.copy()\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_imputed['Fare'] = imputer.fit_transform(df_imputed[['Fare']])\n",
        "print(\"\\nNumber of missing 'Fare' after SimpleImputer:\", df_imputed['Fare'].isnull().sum())"
      ],
      "metadata": {
        "id": "6YVEbQPQRPKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# OPTIONAL: Save processed datasets for comparison\n",
        "# -------------------------------------------------------------------------\n",
        "df_removed.to_csv(\"titanic_removed.csv\", index=False)\n",
        "df_filled_constant.to_csv(\"titanic_filled_constant.csv\", index=False)\n",
        "df_filled_stat.to_csv(\"titanic_filled_stat.csv\", index=False)\n",
        "df_imputed.to_csv(\"titanic_imputed.csv\", index=False)\n",
        "\n",
        "print(\"\\nCleaned datasets saved for comparison.\")"
      ],
      "metadata": {
        "id": "c74aQkFFRXhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# FEATURE ENGINEERING\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# STEP 2: Create Family Size feature\n",
        "# FamilySize = SibSp (siblings/spouses aboard) + Parch (parents/children aboard) + 1 (the passenger themself)\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1"
      ],
      "metadata": {
        "id": "HcW7oVuTSZli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Extract Title from Name\n",
        "# Example: \"Braund, Mr. Owen Harris\" -> \"Mr\"\n",
        "df['Title'] = df['Name'].str.extract(r',\\s*([^\\.]+)\\.')  # Extracts word(s) between ',' and '.'\n",
        "\n",
        "# Optional: Simplify rare titles into common groups\n",
        "df['Title'] = df['Title'].replace(\n",
        "    ['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\n",
        "     'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "df['Title'] = df['Title'].replace({'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs'})\n",
        "\n",
        "print(\"\\nUnique Titles extracted:\", df['Title'].unique())\n",
        "\n"
      ],
      "metadata": {
        "id": "j2jFf0YlSmwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# ENCODING CATEGORICAL VARIABLES\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# STEP 4: Encode 'Title' using Label Encoding (good for ordinal or few categories)\n",
        "label_encoder = LabelEncoder()\n",
        "df['Title_Label'] = label_encoder.fit_transform(df['Title'])\n",
        "\n",
        "# STEP 5: Encode 'Sex' using Label Encoding (binary category: male/female)\n",
        "df['Sex_Label'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# STEP 6: One-Hot Encode 'Embarked' (multiple unordered categories)\n",
        "df = pd.get_dummies(df, columns=['Embarked'], prefix='Embarked')"
      ],
      "metadata": {
        "id": "Md_Yq4WbSnIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# CHECK RESULTS\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nNew features added: FamilySize, Title, Title_Label, Sex_Label\")\n",
        "print(\"One-hot encoded columns for Embarked created.\")\n",
        "print(\"\\nPreview of dataset with new features:\")\n",
        "print(df[['Name','FamilySize','Title','Title_Label','Sex','Sex_Label']].head())"
      ],
      "metadata": {
        "id": "9sLCXrnDSxB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# OPTIONAL: Save updated dataset\n",
        "# -------------------------------------------------------------------------\n",
        "df.to_csv(\"titanic_feature_engineered.csv\", index=False)\n",
        "print(\"\\nFeature-engineered dataset saved as 'titanic_feature_engineered.csv'.\")"
      ],
      "metadata": {
        "id": "M20TvGBIS1Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Choose numeric columns for outlier analysis\n",
        "numeric_cols = ['Age', 'Fare']"
      ],
      "metadata": {
        "id": "U1WLz_UZTJfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 2: Visualize distributions with boxplots and histograms\n",
        "# -------------------------------------------------------------------------\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(10,4))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.boxplot(y=df[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    sns.histplot(df[col], bins=30, kde=True)\n",
        "    plt.title(f'Histogram of {col}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "acGel3CiTWPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 3: Detect outliers using IQR and Z-score methods\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    return data[(data[column] < lower) | (data[column] > upper)]\n",
        "\n",
        "def detect_outliers_zscore(data, column, threshold=3):\n",
        "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
        "    return data.loc[data[column].dropna().index[z_scores > threshold]]\n",
        "\n",
        "for col in numeric_cols:\n",
        "    print(f\"\\n--- {col} ---\")\n",
        "    outliers_iqr = detect_outliers_iqr(df, col)\n",
        "    outliers_zscore = detect_outliers_zscore(df, col)\n",
        "    print(f\"IQR Outliers: {len(outliers_iqr)}\")\n",
        "    print(f\"Z-score Outliers: {len(outliers_zscore)}\")"
      ],
      "metadata": {
        "id": "Usqk-CImTc3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 4: Handle outliers\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# (a) Quantile Capping at 0.98\n",
        "df_capped = df.copy()\n",
        "for col in numeric_cols:\n",
        "    upper_cap = df_capped[col].quantile(0.98)\n",
        "    df_capped[col] = np.where(df_capped[col] > upper_cap, upper_cap, df_capped[col])\n",
        "\n",
        "# (b) Log Transformation (for skewed features like Fare)\n",
        "df_log = df.copy()\n",
        "df_log['Fare'] = np.log1p(df_log['Fare'])  # log1p avoids issues with zero\n",
        "\n",
        "# (c) Row Removal (remove rows with Age outliers based on IQR)\n",
        "df_removed = df.copy()\n",
        "age_outliers = detect_outliers_iqr(df_removed, 'Age').index\n",
        "df_removed = df_removed.drop(age_outliers)\n"
      ],
      "metadata": {
        "id": "QDL4kvyqThqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 5: Compare datasets before and after treatment\n",
        "# -------------------------------------------------------------------------\n",
        "def compare_distributions(original, modified, column, title_suffix):\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    sns.histplot(original[column], bins=30, color='blue', label='Original', alpha=0.5)\n",
        "    sns.histplot(modified[column], bins=30, color='red', label=title_suffix, alpha=0.5)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(f\"{column} Distribution: Original vs {title_suffix}\")\n",
        "    plt.show()\n",
        "\n",
        "# Compare Age and Fare after different treatments\n",
        "compare_distributions(df, df_capped, 'Fare', 'Capped at 0.98 Quantile')\n",
        "compare_distributions(df, df_log, 'Fare', 'Log Transformed')\n",
        "compare_distributions(df, df_removed, 'Age', 'Outliers Removed')\n"
      ],
      "metadata": {
        "id": "oaDghR1VYgF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 6: Explore quantile thresholds\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nExploring quantile thresholds for Fare:\")\n",
        "print(\"0.98 quantile:\", df['Fare'].quantile(0.98))\n",
        "print(\"0.99 quantile:\", df['Fare'].quantile(0.99))"
      ],
      "metadata": {
        "id": "02m8PS01YlB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# STEP 1: Load Titanic dataset (after outlier treatment, if you saved it)\n",
        "df = pd.read_csv(\"titanic_cleaned.csv\")  # <-- replace with your treated dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "WUfz55amZM1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Select numerical features for scaling\n",
        "numeric_cols = ['Age', 'Fare', 'FamilySize']  # Example set; add more if needed"
      ],
      "metadata": {
        "id": "TNQk8SExZQSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 3: Visualize distributions before scaling\n",
        "# -------------------------------------------------------------------------\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.histplot(df[col], bins=30, kde=True)\n",
        "    plt.title(f'Distribution of {col} (Before Scaling)')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Fo8OjD0fZROu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 4: Apply different scalers\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Initialize scalers\n",
        "standard_scaler = StandardScaler()   # Good for normally distributed features\n",
        "minmax_scaler = MinMaxScaler()       # Good for skewed or bounded features\n",
        "\n",
        "# Example decision:\n",
        "# - 'Age' is roughly normal → StandardScaler\n",
        "# - 'Fare' is skewed → MinMaxScaler\n",
        "# - 'FamilySize' is small bounded integers → MinMaxScaler\n",
        "df_scaled = df.copy()\n",
        "\n",
        "# Apply StandardScaler to Age\n",
        "df_scaled['Age_scaled'] = standard_scaler.fit_transform(df[['Age']])\n",
        "\n",
        "# Apply MinMaxScaler to Fare and FamilySize\n",
        "df_scaled['Fare_scaled'] = minmax_scaler.fit_transform(df[['Fare']])\n",
        "df_scaled['FamilySize_scaled'] = minmax_scaler.fit_transform(df[['FamilySize']])"
      ],
      "metadata": {
        "id": "zAviFYFlZa8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 5: Visualize distributions after scaling\n",
        "# -------------------------------------------------------------------------\n",
        "for col in ['Age_scaled', 'Fare_scaled', 'FamilySize_scaled']:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.histplot(df_scaled[col], bins=30, kde=True, color='red')\n",
        "    plt.title(f'Distribution of {col} (After Scaling)')\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 6: Check results\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nPreview of scaled features:\")\n",
        "print(df_scaled[['Age', 'Age_scaled', 'Fare', 'Fare_scaled', 'FamilySize', 'FamilySize_scaled']].head())\n",
        "\n",
        "# Optional: Save scaled dataset\n",
        "df_scaled.to_csv(\"titanic_scaled.csv\", index=False)\n",
        "print(\"\\nScaled dataset saved as 'titanic_scaled.csv'.\")"
      ],
      "metadata": {
        "id": "MXAEhaxYZgQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 2: Identify categorical columns\n",
        "# -------------------------------------------------------------------------\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(\"Categorical columns found:\", categorical_cols)\n",
        "\n",
        "# Example: ['Sex', 'Embarked', 'Title', 'Cabin']"
      ],
      "metadata": {
        "id": "rECrEfFfaDdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 3: Decide encoding strategy\n",
        "# -------------------------------------------------------------------------\n",
        "# - Nominal variables (unordered categories) → One-Hot Encoding\n",
        "#   Examples: Sex, Embarked, Title\n",
        "# - Ordinal variables (ordered categories, if any) → Label Encoding\n",
        "#   Example: a column like 'Education' = Primary < Secondary < Tertiary\n",
        "\n",
        "# For this Titanic dataset, we assume all are nominal (no natural order).\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 4: One-Hot Encoding for nominal variables\n",
        "# -------------------------------------------------------------------------\n",
        "# Use get_dummies() to create dummy variables\n",
        "df_encoded = pd.get_dummies(df, columns=['Sex', 'Embarked', 'Title'], drop_first=True)\n",
        "\n",
        "# drop_first=True removes one category to avoid multicollinearity\n",
        "# Example: for 'Sex', only 'Sex_male' remains (0=female, 1=male)\n",
        "\n"
      ],
      "metadata": {
        "id": "dip_gqe8agZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 5: Label Encoding (if there were ordinal features)\n",
        "# -------------------------------------------------------------------------\n",
        "# Example placeholder for an ordinal column 'ClassLevel'\n",
        "if 'ClassLevel' in df.columns:\n",
        "    label_encoder = LabelEncoder()\n",
        "    df_encoded['ClassLevel'] = label_encoder.fit_transform(df['ClassLevel'])\n",
        "\n"
      ],
      "metadata": {
        "id": "oG3hHiZMai10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 6: Verify encoding\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nPreview of encoded dataset:\")\n",
        "print(df_encoded.head())\n",
        "\n",
        "print(\"\\nEncoded columns now include:\")\n",
        "print([col for col in df_encoded.columns if any(cat in col for cat in ['Sex_', 'Embarked_', 'Title_'])])\n",
        "\n"
      ],
      "metadata": {
        "id": "1auM7bqEakpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# STEP 7: Save final dataset\n",
        "# -------------------------------------------------------------------------\n",
        "df_encoded.to_csv(\"titanic_encoded.csv\", index=False)\n",
        "print(\"\\nFinal encoded dataset saved as 'titanic_encoded.csv'.\")\n"
      ],
      "metadata": {
        "id": "BW6qe-opameT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# STEP 1: Load dataset (after missing value handling and outlier treatment)\n",
        "df = pd.read_csv(\"titanic_cleaned.csv\")  # replace with your processed dataset\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 2: Create Age Groups using pd.cut()\n",
        "# -------------------------------------------------------------------------\n",
        "# Define bins (edges) and corresponding labels\n",
        "age_bins = [0, 12, 18, 60, 100]               # child, teen, adult, senior\n",
        "age_labels = ['Child', 'Teen', 'Adult', 'Senior']\n",
        "\n",
        "# Use pd.cut() to assign each passenger to a group\n",
        "df['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False)\n",
        "\n",
        "# right=False → interval is [ ) so 0 ≤ age < 12 is Child, etc.\n",
        "\n",
        "print(\"\\nUnique age groups created:\", df['AgeGroup'].unique())\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 3: One-Hot Encode AgeGroup\n",
        "# -------------------------------------------------------------------------\n",
        "df_encoded = pd.get_dummies(df, columns=['AgeGroup'], prefix='AgeGroup')\n",
        "\n",
        "# Example: AgeGroup_Child, AgeGroup_Teen, AgeGroup_Adult, AgeGroup_Senior\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 4: Verify new columns\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nPreview of dataset with AgeGroup encoding:\")\n",
        "print(df_encoded[['Age', 'AgeGroup_Child', 'AgeGroup_Teen', 'AgeGroup_Adult', 'AgeGroup_Senior']].head(10))\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# STEP 5: Save updated dataset\n",
        "# -------------------------------------------------------------------------\n",
        "df_encoded.to_csv(\"titanic_agegroup_encoded.csv\", index=False)\n",
        "print(\"\\nDataset with AgeGroup feature saved as 'titanic_agegroup_encoded.csv'.\")\n"
      ],
      "metadata": {
        "id": "uqQjZRLQa5RI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}