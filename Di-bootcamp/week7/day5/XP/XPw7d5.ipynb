{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyH2jZ6JY04L"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 lxml --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Assign the HTML content from the previous cell to html_content\n",
        "html_content = \"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Sports World</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; }\n",
        "        header, nav, section, article, footer { margin: 20px; padding: 15px; }\n",
        "        nav { background-color: #333; }\n",
        "        nav a { color: white; padding: 14px 20px; text-decoration: none; display: inline-block; }\n",
        "        nav a:hover { background-color: #ddd; color: black; }\n",
        "        .video { text-align: center; margin: 20px 0; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "    <header>\n",
        "        <h1>Welcome to Sports World</h1>\n",
        "        <p>Your one-stop destination for the latest sports news and videos.</p>\n",
        "    </header>\n",
        "\n",
        "    <nav>\n",
        "        <a href=\"#football\">Football</a>\n",
        "        <a href=\"#basketball\">Basketball</a>\n",
        "        <a href=\"#tennis\">Tennis</a>\n",
        "    </nav>\n",
        "\n",
        "    <section id=\"football\">\n",
        "        <h2>Football</h2>\n",
        "        <article>\n",
        "            <h3>Latest Football News</h3>\n",
        "            <p>Read about the latest football matches and player news.</p>\n",
        "            <div class=\"video\">\n",
        "                <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/football-video-id\" frameborder=\"0\" allowfullscreen>\n",
        "                </iframe>\n",
        "            </div>\n",
        "        </article>\n",
        "    </section>\n",
        "\n",
        "    <section id=\"basketball\">\n",
        "        <h2>Basketball</h2>\n",
        "        <article>\n",
        "            <h3>NBA Highlights</h3>\n",
        "            <p>Watch highlights from the latest NBA games.</p>\n",
        "            <div class=\"video\">\n",
        "                <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/basketball-video-id\" frameborder=\"0\" allowfullscreen>\n",
        "                </iframe>\n",
        "            </div>\n",
        "        </article>\n",
        "    </section>\n",
        "\n",
        "    <section id=\"tennis\">\n",
        "        <h2>Tennis</h2>\n",
        "        <article>\n",
        "            <h3>Grand Slam Updates</h3>\n",
        "            <p>Get the latest updates from the world of Grand Slam tennis.</p>\n",
        "            <div class=\"video\">\n",
        "                <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tennis-video-id\" frameborder=\"0\" allowfullscreen></iframe>\n",
        "            </div>\n",
        "        </article>\n",
        "    </section>\n",
        "\n",
        "    <footer>\n",
        "        <form action=\"mailto:contact@sportsworld.com\" method=\"post\" enctype=\"text/plain\">\n",
        "            <label for=\"name\">Name:</label><br>\n",
        "            <input type=\"text\" id=\"name\" name=\"name\"><br>\n",
        "            <label for=\"email\">Email:</label><br>\n",
        "            <input type=\"email\" id=\"email\" name=\"email\"><br>\n",
        "            <label for=\"message\">Message:</label><br>\n",
        "            <textarea id=\"message\" name=\"message\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
        "            <input type=\"submit\" value=\"Send\">\n",
        "        </form>\n",
        "    </footer>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# --- Step 2: Create a BeautifulSoup object ---\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")"
      ],
      "metadata": {
        "id": "MDIRFVtkaNEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Find the title of the webpage ---\n",
        "title = soup.title.string"
      ],
      "metadata": {
        "id": "Vtyr6ZaNaXod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Extract all paragraphs (<p> tags) ---\n",
        "paragraphs = [p.get_text() for p in soup.find_all('p')]"
      ],
      "metadata": {
        "id": "Q8yhVNOiaaqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Retrieve all links (<a href=\"\">) ---\n",
        "links = [a['href'] for a in soup.find_all('a', href=True)]"
      ],
      "metadata": {
        "id": "m8zXpcO3aczT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Display results ---\n",
        "print(\"Page Title:\", title)\n",
        "print(\"Paragraphs:\", paragraphs)\n",
        "print(\"Links:\", links)"
      ],
      "metadata": {
        "id": "VLl1X8ceafJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2 : Scraping robots.txt from Wikipediaё\n",
        "import requests\n",
        "\n",
        "url = \"https://en.wikipedia.org/robots.txt\"\n",
        "\n",
        "# Add a browser-like User-Agent header\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/118.0.5993.70 Safari/537.36\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\" Wikipedia robots.txt content:\\n\")\n",
        "    print(response.text)\n",
        "else:\n",
        "    print(f\" Failed to retrieve robots.txt. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "O6WHjIgn5C9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3 : Extracting Headers from Wikipedia’s Main Page\n",
        "# URL of the Wikipedia page\n",
        "url = \"https://en.wikipedia.org/wiki/Wikipedia\"\n",
        "\n",
        "# Add headers to avoid 403 Forbidden\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/118.0.5993.70 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Fetch the page content\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Parse HTML with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find all header tags (h1–h6)\n",
        "    headers = []\n",
        "    for i in range(1, 7):\n",
        "        for tag in soup.find_all(f\"h{i}\"):\n",
        "            headers.append((f\"h{i}\", tag.get_text(strip=True)))\n",
        "\n",
        "    # Display the results\n",
        "    print(\"Header tags found on the page:\\n\")\n",
        "    for tag_name, text in headers:\n",
        "        print(f\"{tag_name}: {text}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "tH-_vrv1gKVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Exercise 4 : Checking for Page Title\n",
        "# Find all header tags (h1–h6)\n",
        "all_headers = []\n",
        "for i in range(1, 7):\n",
        "    tags = soup.find_all(f\"h{i}\")\n",
        "    for tag in tags:\n",
        "        all_headers.append((f\"h{i}\", tag.get_text(strip=True)))\n",
        "\n",
        "# Display results\n",
        "print(\"Header tags found on the page:\\n\")\n",
        "for tag_name, text in all_headers:\n",
        "    print(f\"{tag_name}: {text}\")\n"
      ],
      "metadata": {
        "id": "z9jZ4DKehhWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4 : Checking for Page Title\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Suppose 'html_content' already contains the loaded HTML page\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Example Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Hello World!</h1>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Create BeautifulSoup object\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "# Check if a title tag exists\n",
        "title_tag = soup.title\n",
        "\n",
        "if title_tag and title_tag.string:\n",
        "    print(\"The page contains a title:\", title_tag.string)\n",
        "else:\n",
        "    print(\"The page does not contain a title tag.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci1J4ByoiYfj",
        "outputId": "22ade8bd-2691-4ba8-843f-2c37d04cf6c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The page contains a title: Example Page\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5 : Analyzing US-CERT Security Alerts\n",
        "import datetime\n",
        "\n",
        "def count_cisa_alerts(year=None, page_limit=10):\n",
        "    if year is None:\n",
        "        year = datetime.datetime.now().year\n",
        "    base_url = 'https://www.cisa.gov/news-events/cybersecurity-advisories'\n",
        "    # this query parameter filters to “alert” type (advisory_type=93)\n",
        "    params = {\n",
        "        'f[0]': 'advisory_type:93',\n",
        "        'page': 0\n",
        "    }\n",
        "    count = 0\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/118.0.5993.70 Safari/537.36\"\n",
        "    })\n",
        "    for page in range(page_limit):\n",
        "        params['page'] = page\n",
        "        resp = session.get(base_url, params=params)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}, status code {resp.status_code}\")\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        # find all entries dated in the given year\n",
        "        articles = soup.select('article.views-row')  # adjust selector if needed\n",
        "        if not articles:\n",
        "            break\n",
        "        for art in articles:\n",
        "            # extract date\n",
        "            date_tag = art.select_one('.views-field-created .field-content')\n",
        "            if not date_tag:\n",
        "                continue\n",
        "            # parse date somewhere like \"Sep 23, 2025\"\n",
        "            try:\n",
        "                date_str = date_tag.get_text(strip=True)\n",
        "                date_obj = datetime.datetime.strptime(date_str, \"%b %d, %Y\")\n",
        "            except Exception:\n",
        "                continue\n",
        "            if date_obj.year == year:\n",
        "                count += 1\n",
        "            elif date_obj.year < year:\n",
        "                # assuming entries are sorted by date descending, we can stop\n",
        "                return count\n",
        "        # next page\n",
        "    return count\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    year = datetime.datetime.now().year\n",
        "    num_alerts = count_cisa_alerts(year=year, page_limit=20)\n",
        "    print(f\"Number of CISA “Alert”-type entries for {year}: {num_alerts}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2iiSRkEi2Vx",
        "outputId": "d1213b6e-b70e-45f1-91fa-85cbc13d9798"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CISA “Alert”-type entries for 2025: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 6 : Scraping Movie Details\n",
        "import random\n",
        "import time\n",
        "\n",
        "LIST_URL = \"https://www.imdb.com/list/ls091294718/\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/118.0.5993.70 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def get_movie_links_from_list():\n",
        "    \"\"\"Fetch the list page and return all movie detail URLs from that list.\"\"\"\n",
        "    resp = requests.get(LIST_URL, headers=HEADERS)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    links = []\n",
        "    for a in soup.select(\"h3.lister-item-header a\"):\n",
        "        href = a.get(\"href\")\n",
        "        if href and href.startswith(\"/title/\"):\n",
        "            links.append(\"https://www.imdb.com\" + href)\n",
        "    return links\n",
        "\n",
        "def extract_movie_details(url):\n",
        "    \"\"\"Given a movie detail page URL, return (title, year, summary).\"\"\"\n",
        "    resp = requests.get(url, headers=HEADERS)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # Title and year\n",
        "    title_tag = soup.find(\"h1\")\n",
        "    if not title_tag:\n",
        "        return None\n",
        "    title_text = title_tag.get_text(strip=True)\n",
        "    # IMDb often has the year in a span or appended; clean up:\n",
        "    year_span = soup.find(\"span\", id=\"titleYear\")\n",
        "    year = year_span.get_text(strip=True).strip(\"()\") if year_span else \"\"\n",
        "\n",
        "    # Summary / plot\n",
        "    summary_tag = soup.select_one(\"div.summary_text\")\n",
        "    if not summary_tag:\n",
        "        # alternative: look for <span class=\"GenresAndPlot__TextContainerBreakpointXS_TO_M-cum89p-0 dcFkRD\"> etc\n",
        "        summary = \"\"\n",
        "    else:\n",
        "        summary = summary_tag.get_text(strip=True)\n",
        "\n",
        "    return {\n",
        "        \"title\": title_text,\n",
        "        \"year\": year,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    links = get_movie_links_from_list()\n",
        "    print(f\"Found {len(links)} movies in the list.\")\n",
        "    chosen = random.sample(links, min(10, len(links)))\n",
        "    results = []\n",
        "    for idx, url in enumerate(chosen, start=1):\n",
        "        print(f\"Fetching {idx}/10: {url}\")\n",
        "        details = extract_movie_details(url)\n",
        "        if details:\n",
        "            results.append(details)\n",
        "        else:\n",
        "            print(\"Failed to extract details for:\", url)\n",
        "        time.sleep(1)  # polite delay\n",
        "    print(\"\\n=== Selected Movies ===\")\n",
        "    for movie in results:\n",
        "        print(f\"Title: {movie['title']}\")\n",
        "        print(f\"Year:  {movie['year']}\")\n",
        "        print(f\"Summary: {movie['summary']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUbC8A1ej4ER",
        "outputId": "1de425a9-49fe-4c16-8cba-938c03c90f57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 movies in the list.\n",
            "\n",
            "=== Selected Movies ===\n"
          ]
        }
      ]
    }
  ]
}